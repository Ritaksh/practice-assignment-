{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWhat is an unsupervised learning approach? Why is it needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning  algorithms to identify patterns in data sets containing data points that are neither classified nor labeled.\n",
    "\n",
    "In some case we didn't get labeled data or we don't know about the shape,pattern or behaviour of the output  se unsupervised machine learning algorithm help to find the better result in this kind approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tWhat is clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustring is unsupervised approach which finds the pattern in a  collection of unlabelled dataset .\n",
    "a cluster is a collection of objects which are similar  amongest themselves and are disimillar to the object belogning  to a differnet group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3.\tHow do clustering and classification differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "both techniques have certain simliarities  but clasification uses predefined classes in which objects are assigned\n",
    "\n",
    "while clustering identifies simlarities between objects, which group according to those characterstics in comman and which differntiate from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tWhat are the various applications of clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For customer segmentation: You can cluster your customers based on their purchases,their activity on your website, and so on. This is useful to understand who your customers are and what they need, so you can adapt your products and marketing campaigns to each segment. For example, this can be useful in recommender systems to suggest content that other users in the same cluster enjoyed.\n",
    "\n",
    "For data analysis: When analyzing a new dataset, it is often useful to first discover clusters of similar instances, as it is often easier to analyze clusters separately.\n",
    "\n",
    "As a dimensionality reduction technique: Once a dataset has been clustered, it is usually possible to measure each instance’s affinity with each cluster (affinity is any measure of how well an instance fits into a cluster). Each instance’s feature vector x can then be replaced with the vector of its cluster affinities. If there are k clusters, then this vector is k dimensional. This is typically much lower dimensional than the original feature vector, but it can preserve enough information for further processing.\n",
    "\n",
    "For anomaly detection (also called outlier detection): Any instance that has a low affinity to all the clusters is likely to be an anomaly. For example, if you have clustered the users of your website based on their behavior, you can detect users with unusual behavior, such as an unusual number of requests per second, and so on. Anomaly detection is particularly useful in detecting defects in manufacturing, or for fraud detection.\n",
    "\n",
    "For semi-supervised learning: If you only have a few labels, you could perform clustering and propagate the labels to all the instances in the same cluster. This can greatly increase the amount of labels available for a subsequent supervised learning algorithm, and thus improve its performance. .\n",
    "\n",
    "For search engines: For example, some search engines let you search for images that are similar to a reference image. To build such a system, you would first apply a clustering algorithm to all the images in your database: similar images would end up in the same cluster. Then when a user provides a reference image, all you need to do is to find this image’s cluster using the trained clustering model, and you can then simply return all the images from this cluster.\n",
    "\n",
    "To segment an image: By clustering pixels according to their color, then replacing each pixel’s color with the mean color of its cluster, it is possible to reduce the number of different colors in the image considerably. This technique is used in many object detection and tracking systems, as it makes it easier to detect the contour of each object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tHow does clustering play a role in supervised learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If you only have a few labels, you could perform clustering and propagate the labels to all the instances in the same cluster. This can greatly increase the amount of labels available for a subsequent supervised learning algorithm, and thus improve its performance. ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tWhat are the requirements to be met by a clustering algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it should be scalable\n",
    "\n",
    "it should be able to deal with attributes of  different types\n",
    "\n",
    "it should be able to discover arbitrary shape clusters\n",
    "\n",
    "it should have an in built ability to deal with noise and outlier\n",
    "\n",
    "the cluster should not vary  with the  order of input  records\n",
    "\n",
    "it shoud be able  to handle data of high dimensions\n",
    "\n",
    "it should be easy to interpret and use\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tDiscuss the different approaches for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two approahes for clustering:\n",
    "\n",
    "Agglomerative Approach : in this first consider all points individual cluster and then find the  similarity between two points,put them together in a cluster then it goes on finding similar points and clusters untill there is only one cluster left i.e all points belong to a big cluster.this is also  called the bottom up approach.\n",
    "\n",
    "Divisve : it is opposite of the agglomerative approach.it first consider all the points to  be a part of one big cluster and in the subsequent step tries to find out points/clusters which are least similar to  each other and then breaks the bigger cluster into smaller ones.\n",
    " this continues untill there are as many clusters as there are datapoints.this  also called the topdown approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tWhat is WCSS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an ideal way to figure out the number of clusters would be to calculate the within sum of squares(WCSS)\n",
    "\n",
    "wcss is the sum of squares of the  distances of each data point in all clusters  to their respectives centroids.\n",
    "\n",
    "wcss = sumitation(sumitation distances(d,Ck)^2\n",
    "\n",
    "where Ck is centroids and d is the data point in each cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tDiscuss the elbow method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we need to provide the value of cluster in cluster algorithm, with the help of elbow method we find the optimum value of k.\n",
    " \n",
    "elbow method is based on the relationship between the WCSS and the number of the clusters.\n",
    "it uis observed that the first with an increase in the number of clusters WCSS  decrease steeply and then after a certain number of clusters the drop in WCSS  is not  that prominent.\n",
    "the point after which the graph betwen WCSS and the number of cluster become comparatively smoother is termed as elbow and the number of the  cluster at that point are the optimum number of clusters as even after increasing the clusters after that point the variation is not decreasing by much i.e we have accounted for almost all the dissimilarity in the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.\tWhat is the significance of ‘K’ in K-Means and how is it calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means is clutering apporach in which the data is grouped into  k Distinct non overlapping cluters based on their distances from the K centres.\n",
    "the value of K needs to be specified  first and the algorithm assign the points to exaclty one cluster\n",
    " \n",
    "the value of k is calculated by elbow method with is the plotted by relationship in between WCSS and number of  cluster allocated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.\tDiscuss the step by step implementation of K-Means Clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "randomly assign k numbers of clusters or k centres\n",
    "\n",
    "calculate the distance of all the points from all the k centres and allocate the point to cluster based on the shortest distance.the model's interia is the means squared distance between each instance and its closest centroid.the goal is to have a model with the lowest interia.\n",
    "\n",
    "once all the points are assigned to clusters recompute the centroids\n",
    "\n",
    "repeat the  upper steps utill the locations of the centroids stop changing and the cluster allocation of the points becomes constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.\tWhat are the challenges with K-Means?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ned to specify the numbers of custers beforehead\n",
    "\n",
    "it is required to run the algorithm multiple times to avoid a sub optimal solutions\n",
    "\n",
    "K-means does not behave very well when the cluster have varying sizes, differnt densities, or non spherical shapes\n",
    "\n",
    "the cluter some times vary based on the intial choice of the centroids.\n",
    " \n",
    "these are the challanges facing in the  simple k-means cluster algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.\tDiscuss the various improvements in K-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "improvement to the K-Means algorithm, called K-Means++, was proposed in a 2006 paper by David Arthur and Sergei Vassilvitskii. They introduced a smarter initialization step that tends to select centroids that are distant from one another, and this makes the K-Means algorithm much less likely to converge to a suboptimal solution.\n",
    "\n",
    "Another important improvement to the K-Means algorithm was proposed in a 2003 paper by Charles Elkan.It considerably accelerates the algorithm by avoiding many unnecessary distance calculations: this is achieved by exploiting the triangle inequality (i.e., the straight line is always the shortest; in a triangle with sides a,b and c=> a+b>c) and by keeping track of lower and upper bounds for distances between instances and centroids.\n",
    "\n",
    "Yet another important variant of the K-Means algorithm was proposed in a 2010 paper by David Sculley. Instead of using the full dataset at each iteration, the algorithm is capable of using mini-batches, moving the centroids just slightly at each iteration. This speeds up the algorithm typically by a factor of 3 or 4 and makes it possible to cluster huge datasets that do not fit in memory. Scikit-Learn implements this algorithm in the MiniBatchKMeans class. You can just use this class like the KMeans class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Clustering:\n",
    "14.\tDiscuss the agglomerative and divisive clustering approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agglomerative Approach : in this first consider all points individual cluster and then find the  similarity between two points,put them together in a cluster then it goes on finding similar points and clusters untill there is only one cluster left i.e all points belong to a big cluster.this is also  called the bottom up approach.\n",
    "\n",
    "Divisve : it is opposite of the agglomerative approach.it first consider all the points to  be a part of one big cluster and in the subsequent step tries to find out points/clusters which are least similar to  each other and then breaks the bigger cluster into smaller ones.\n",
    " this continues untill there are as many clusters as there are datapoints.this  also called the topdown approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15.\tWhat are dendrograms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dendrogram is a diagram that shows the hierarchical relationship between objects. It is commonly created as an output from hierarchical clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16.\tDiscuss the Hierarchical clustering in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are take bottom-up approach in hierarchical clutering.we start by defining any sort  similarity between the datapoints.\n",
    "generally we consider the Euclidance distance.the points which are closer to each are more similar than the points which are farther away.\n",
    "the algorithm starts with considering all points as seperate clusters and then grouping pints together to form clusters.\n",
    "\n",
    "algorithm.\n",
    "\n",
    "begin with n observation and measure the Euclidence distance of all the n(n-1)/2 pairwise dissimalrities. treat each observation as its own cluster.intially we have n cluster.\n",
    "\n",
    "Compare all the distancea and put two near point in the same cluster.the dissimlarity between these clusters indicates the height of the dendogram at which the fusion line should be placed.\n",
    "\n",
    "compute the new pairwise inter-cluster dissimalrities among the remaining clusters\n",
    "repeat steps 2 and 3 till we have only one cluster left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17.\tDiscuss the various linkage methods for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linkage clustering is usually based on distance only.Based on pairwise distances, we can now compute a linkage matrix.\n",
    "\n",
    "different linkage  methods baed on distance calculation\n",
    "\n",
    "single linkage = cluster distance is based on smallest pairwise linkage\n",
    "\n",
    "complete linkage = cluster distance is based on largest pairwise linkage\n",
    "\n",
    "average linkage = cluster distance is based on average pairwise linkage\n",
    "\n",
    "centroid linkage = distance between the centroids of the clusters\n",
    "\n",
    "ward linkage = cluster criteria is minimize the variance in the cluster i.e WCSS distance\n",
    "\n",
    "Single linkage : Minimal intercluster dissimilarity.\n",
    "cluster distance is the smallest distance between any point in cluster 1 and any point in cluster 2\n",
    "highly sensitive to outliers when forming flat clusters\n",
    "works well for low-noise data with an unusual structure\n",
    "\n",
    "Complete Linkage: Maximal intercluster dissimilarity.cluster distance is the largest distance between any point in cluster 1 and any point in cluster 2\n",
    "less sensitive to outliers than single linkage.\n",
    "\n",
    "Average Linkage: Mean intercluster dissimilarity.cluster distance is the average distance of all pairs of points in clusters 1 and 2.\n",
    "\n",
    "Centroid Linkage: The dissimilarity between the centroid for cluster A (a mean vector of length p) and the centroid for cluster B. Centroid linkage can result in undesirable inversions.\n",
    "cluster distance is the distance of the centroids of both clusters\n",
    "\n",
    "Ward linkage: Ward's minimum variance criterion minimizes the total within-cluster variance. To implement this method, at each step find the pair of clusters that leads to minimum increase in total within-cluster variance after merging.\n",
    "based on minimizing a variance criterion before and after merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18.\tDiscuss the differences between K-Means and Hierarchical clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering can’t handle big data well but K Means clustering can. This is because the time complexity of K Means is linear i.e. O(n) while that of hierarchical clustering is quadratic i.e. O(n2).\n",
    "\n",
    "In K Means clustering, since we start with random choice of clusters, the results produced by running the algorithm multiple times might differ. While results are reproducible in Hierarchical clustering.\n",
    "\n",
    "K Means is found to work well when the shape of the clusters is hyper spherical (like circle in 2D, sphere in 3D).\n",
    "\n",
    "K Means clustering requires prior knowledge of K i.e. no. of clusters you want to divide your data into. But, you can stop at whatever number of clusters you \n",
    "\n",
    "\"find appropriate in hierarchical clustering by interpreting the dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN\n",
    "19.\tDiscuss the basic terms used in DBSCAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon : this is called eps. this the distance till which we look fot the neighouring points(radius)\n",
    "\n",
    "Min_point: the minimun number of points specified  by the user.\n",
    "\n",
    "Core points : if the number of point inside the eps is greater than or equal to the min point then it is called core point.\n",
    "\n",
    "Border point: if the number of points inside the eps is less than min points and it lies with the eps radius of another core point , then it is called boarder point\n",
    "\n",
    "Noise : A point which is neither a core point nor a boarder point is a noise point.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20.\tDiscuss the step by step implementation of DBSCAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm Steps:\n",
    "\n",
    "The algorithm starts with a random point in the dataset which has not been visited yet and its neighbouring points are identified based on the eps value.\n",
    "\n",
    "If the point contains greater than or equal points than the min_pts, then the cluster formation starts and this point becomes a _core point_, else it’s considered as noise. The thing to note here is that a point initially classified as noise can later become a border point if it’s in the eps radius of a core point.\n",
    "\n",
    "If the point is a core point, then all its neighbours become a part of the cluster. If the points in the neighbourhood turn out to be core points then their neighbours are also part of the cluster.\n",
    "\n",
    "Repeat the steps above until all points are classified into different clusters or noises.\n",
    "\n",
    "This algorithm works well if all the clusters are dense enough, and they are well separated by low-density regions.\n",
    "\n",
    "In short, DBSCAN is a very simple yet powerful algorithm, capable of identifying any number of clusters, of any shape, it is robust to outliers, and it has just two hyper parameters(eps and min_samples). However, if the density varies significantly across the clusters, it can be impossible for it to capture all the clusters properly. Moreover, its computational complexity is roughly O(m log m), making it pretty close to linear with regards to the number of instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cluster Evaluation\n",
    "21.\tWhat are the aspects of cluster validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a good cluer will have  High inter-class similarity\n",
    "                        Low intra-class similarity\n",
    "    \n",
    "Aspects- External : Comapare the cluster to the ground truth.\n",
    "internal : Evaluating the cluster without reference to external data\n",
    "Reliabilty : the cluster are not formed by chance,some statistical framework can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22.\tWhat is a confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a confusion matrix, also known as an error matrix, confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23.\tWhat is Jaccard’s coefficient?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jaccard coefficient measures similarity between finite sample sets, and is defined as the size of the intersection divided by the size of the union of the sample sets:\n",
    "\n",
    "N: Number of objects in the data P:  𝑃1,𝑃2,…,𝑃𝑚  the set of ground truth clusters C:  𝐶1,𝐶2,…𝐶𝑛  the set of clusters formed by the algorithm\n",
    "\n",
    "The Incidence Matrix: N* N matrix\n",
    "\n",
    "𝑃𝑖𝑗=1  if the two points  𝑂𝑖  and  𝑂𝑗  belong to the same cluster in the ground truth else  𝑃𝑖𝑗=0 \n",
    "\n",
    "𝐶𝑖𝑗=1  if the two points  𝑂𝑖  and  𝑂𝑗  belong to the same cluster in the cluster else  𝐶𝑖𝑗=0 \n",
    "\n",
    "Now there can be the following scenarios:\n",
    "\n",
    "𝐶𝑖𝑗=𝑃𝑖𝑗=1  --> both the points belong to the same cluster for both our algorithm and ground truth(Agree)--- SS\n",
    "\n",
    "𝐶𝑖𝑗=𝑃𝑖𝑗=0  --> both the points don’t belong to the same cluster for both our algorithm and ground truth(Agree)--- DD\n",
    "\n",
    "𝐶𝑖𝑗=1𝑏𝑢𝑡𝑃𝑖𝑗=0  --> The points belong in the same cluster for our algorithm but in different clusters for the ground truth (Disagree)---- SD\n",
    "\n",
    "𝐶𝑖𝑗=0𝑏𝑢𝑡𝑃𝑖𝑗=1  --> The points don’t belong in the same cluster for our algorithm but in same clusters for the ground truth (Disagree)----DS\n",
    "\n",
    "Jaccard Coeeficient= 𝑆𝑆(𝑆𝑆+𝑆𝐷+𝐷𝑆)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24.\tWhat is Rand Index?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is a measure of the similarity between two data clusterings. A form of the Rand index may be defined that is adjusted for the chance grouping of elements, this is the adjusted Rand index.\n",
    "\n",
    "Rand Index=  𝑇𝑜𝑡𝑎𝑙𝐴𝑔𝑟𝑒𝑒𝑇𝑜𝑡𝑎𝑙𝐼𝑛𝑠𝑡𝑎𝑛𝑐𝑒𝑠=(𝑆𝑆+𝐷𝐷)(𝑆𝑆+𝐷𝐷+𝐷𝑆+𝑆𝐷) \n",
    "\n",
    "The disadvantage of this is that it could be dominated by DD.\n",
    "\n",
    "A higher value of Rand Index and Jaccard's coefficient mean that the clusters generated by our algorithm mostly agree to the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25.\tWhat is the entropy of a cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy Entropy of Cluster i, given by  𝑒𝑖=−∑𝑝𝑖𝑗𝑙𝑜𝑔(𝑝𝑖𝑗) \n",
    "\n",
    "For the entire clustering algorithm, the entropy can be given as:  𝑒=∑𝑚𝑖𝑛𝑒𝑖"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26.\tDiscuss the purity of a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purity The purity is the total percentage of data points clustered correctly.\n",
    "\n",
    "The purity of cluster i, given by  𝑝𝑖=𝑚𝑎𝑥(𝑝𝑖𝑗) \n",
    "\n",
    "And for the entire cluster it is:  𝑝(𝐶)=∑𝑝𝑖𝑛\n",
    "    \n",
    "A high value of purity score means that our clustering algorithm performs well against the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27.\tWhat are cohesion and compression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the methods use to measure the quality of clusters without external references.\n",
    "\n",
    "Cohesion: How closely the objects in the same cluster are related to each other. It is the within-cluster sum of squared distances. It is the same metric that we used to calculate for the K-Means algorithm.  𝑊𝐶𝑆𝑆=∑∑(𝑥−𝑚𝑖)^2\n",
    "\n",
    "Separation: How different the objects in different clusters are and how distinct a well-separated cluster is from other clusters. It is the between cluster sum of squared distances.  𝐵𝑆𝑆=∑𝐶𝑖(𝑚−𝑚𝑖)2 \n",
    "Where C is the size of the individual cluster and m is the centroid of all the data points.\n",
    "\n",
    "Note: BSS+WSS is always a constant.\n",
    "\n",
    "The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). ... The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
    "\n",
    "The silhouette can be calculated as:\n",
    "\n",
    " S(x) = b(x) - a(x)/ max{a(x),b(x)}\n",
    "\n",
    "Where a(x) is the avarage distance of x from all the other points in the same cluster and b(x) is the avarage distance of x from all the other points in the other clusters.\n",
    "\n",
    "And the Silhoeutte coefficient is given by:\n",
    "\n",
    "𝑆𝐶=1𝑁∑𝑆(𝑥)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28.\tWhat are the steps for AWS deployment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the project structure\n",
    "\n",
    "Create the 'app.py' file\n",
    "\n",
    "Go to azure portal and create an account \n",
    "\n",
    "Go to the Azure account and create a web app.\n",
    "\n",
    "Provide the app name, resource group(create new if necessary), runtime stack(Python 3.7), region, select the 1 GB size, which is free to use. Click Review+create to create the web app\n",
    "\n",
    "Once the deployment is completed, open the app and go to the ‘Deployment Center’ option. Select ‘local git’ for source control and click continue\n",
    "\n",
    "Select the kudo ‘App service build provider’ as the build provider and click contin\n",
    "\n",
    "Click ‘Finish’ to complete the setup.\n",
    "\n",
    "Go to the overview section of the app, and the Git link now will be visible.\n",
    "\n",
    "Go to ‘Deployment Credentials’ in deployment center and copy the username and password. These will be required when doing the final push to the remote git repository.\n",
    "\n",
    "Open a command prompt and navigate to your project folder.\n",
    "\n",
    "Run git init to initialise an empty git repository\n",
    "\n",
    "Create a new remote git alias using the command: git remote add\n",
    "\n",
    "Use git add . to add all the files to the local git repository.\n",
    "\n",
    "Use git commit -m \"first commit\" to commit the code to the git repo.\n",
    "\n",
    "Push the code to the remote repo using: git push master –f\n",
    "\n",
    "This prompts for a username and password. Provide the same credentials as copied in the step above.\n",
    "\n",
    "After deployment, from the ‘overview’ section, copy the URL and paste into any web API test to see the application running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29.\tWhat difficulties did you face while deploying to AWS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As first time deploying code in azure portal ,simply follow the deployment steps and run a web api smoothly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
