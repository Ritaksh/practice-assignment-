{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)\tHow decision tree works for a regression problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " in decision tree regression we try to divide the data values into seperate non overlapping regions .\n",
    " then the average response of the prediction is equal to the final outcome.\n",
    "  \n",
    "  equation or this is - EE (Yi - meanY)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)\tWhy Recursive binary splitting is called Greedy Approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A greedy approach is used to divide the space called recursive binary splitting.\n",
    "\n",
    "This is a numerical procedure where all the values are lined up and different split points are tried and tested using a cost function. The split with the best cost (lowest cost because we minimize cost) is selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)\tWhat do you understand by Greedy approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we try to divide the X values into j regions, but it is very expensive in terms of computational time to try to fit every set of X values into j regions. Thus, decision tree opts for a top-down greedy approach in which nodes are divided into two regions based on the given condition, i.e. not every node will be split but the ones which satisfy the condition are split into two branches. It is called greedy because it does the best split at a given step at that point of time rather than looking for splitting a step for a better tree in upcoming steps. It decides a threshold value to divide the observations into different regions Similarly, more regions are split out of the regions created above based on some condition with the same logic. This continues until a stopping criterion (predefined) is achieved. Once all the regions are split, the prediction is made based on the mean of observations in that region.\n",
    "greedy algorithm greedily searches for an optimum split at the top level, then repeats the process at each level. It does not check whether or not the split will lead to the lowest possible impurity several levels down. A greedy algorithm often produces a reasonably good solution, but it is not guaranteed to be the optimal solution.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4)\tWhat is Pruning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tree pruning is method for trimming the full tree o reduce the complexity and variance  in the data. just as we regularized linear regression model ,we can also regularise the decision tree model by adding a new term.\n",
    "equation  - E E (Yi - Yrm)**2 + alpha|T| \n",
    " where T is subtree of the full tree T0 and alpha is the non-negative tuning parameter which limit the MSE with an increase in tree length.\n",
    " by cross validation the va;ue of alpha and T are selected  for which gives  the low error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)\tWhat’s the difference between pre pruning and post pruning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " pre pruning are forward pruning stops  the non significant  branches  from generating.\n",
    " it uses a condition to decide when should it terminate  spliting of some of the branches prematurely as the tree is generated\n",
    " \n",
    " post pruning  are backward pruning.is the process where the decision tree is generated first and then the non significant  branches are removed.\n",
    " cross validation  set of data is used to check the effect of pruning and tests whether expanding  a node will make an improvement or not.\n",
    "  if any improvement is there then we continue by expanding that node else if there is reduction in accuracy then the node not be expanded and should be converted in a leaf node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6)\tWhat is Entropy? How is it calculated?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Entropy is the measure of randomness in the data. it gives the impurity present in the dataset.\n",
    " \n",
    "  entropy equation  = -p log2(p)\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7)\tWhat is Gini Impurity?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labelled if it was randomly labelled according to the distribution of labels in the subset.\n",
    "\n",
    "calculated by multiplying the probability that a given observation is classified into the correct class and sum of all the probabilities when that particular observation is classified into the wrong class.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8)\tWhat do you understand by Information Gain? How does it help in tree building?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information gain calculates the decrease in entropy after splitting a node. It is the difference between entropies before and after the split. The more the information gain, the more entropy is removed.\n",
    " equation  - I.G  = old entropy - new entropy\n",
    " \n",
    "On the basis of IG which node has maximuM Information gain is slected as root node so the tree is built as this step is taken till the leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "9)\tHow does node selection take place while building a tree?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " nodes is selected on the basic of gini indexing or information gain basic on  decision tree algorithm\n",
    "  for gini indexing - which node has less gini indes is selected as root node\n",
    "  \n",
    "  for information gain -  which node has maximum informaion gain is selected as root node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10)\tWhat are different algorithms available for decision tree?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID3 (Iterative Dichotomiser) : It is one of the algorithms used to construct decision tree for classification. It uses Information gain as the criteria for finding the root nodes and splitting them. It only accepts categorical attributes.\n",
    "\n",
    "C4.5 : It is an extension of ID3 algorithm, and better than ID3 as it deals both continuous and discreet values.It is also used for classfication purposes.\n",
    "\n",
    "Classfication and Regression Algorithm(CART) : It is the most popular algorithm used for constructing decison trees. It uses ginni impurity as the default calculation for selecting root nodes, however one can use \"entropy\" for criteria as well. This algorithm works on both regression as well as classfication problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11)\tWhat’s the main difference between Gini Impurity and Entropy on the basis of computation time?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per computation time the entropy  criteria is take more time than gini impurity method as it take log converstion if probability of each instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12)\tWhat are the disadvantages and advantages of using a Decision Tree?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages of Decision Tree:\n",
    "It can be used for both Regression and Classification problems.\n",
    "Decision Trees are very easy to grasp as the rules of splitting is clearly mentioned.\n",
    "Complex decision tree models are very simple when visualized. It can be understood just by visualising.\n",
    "Scaling and normalization are not needed.\n",
    "\n",
    "\n",
    "Disadvantages of Decision Tree:\n",
    "A small change in data can cause instability in the model because of the greedy approach.\n",
    "Probability of overfitting is very high for Decision Trees.\n",
    "It takes more time to train a decision tree model than other classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13)\tHow do you deploy model in Heroku?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps for heroku deployement :-\n",
    "\n",
    "create  a heroku account\n",
    "\n",
    "After installing the Heroku CLI, Open a command prompt window and navigate to your project folder.\n",
    "\n",
    "Type the command heroku login to login to your heroku account.\n",
    "After logging in to Heroku, enter the command heroku create to create a heroku app. \n",
    "\n",
    "It will give you the URL of your Heroku app after successful creation. Or alternatively, you can go to the heroku website and create an app directly.\n",
    "\n",
    "Before deploying the code to the Heroku cloud, we need to commit the changes to the git repository.\n",
    "\n",
    "Type the command git init to initialize a local git repository.\n",
    "\n",
    "Enter the command git status to see the uncommitted changes.\n",
    "\n",
    "Enter the command git add . to add the uncommitted changes to the local repository.\n",
    "\n",
    "Enter the command git commit -am \"make it better\" to commit the changes to the local repository.\n",
    "\n",
    "Enter the command git push heroku master to push the code to the heroku cloud.\n",
    "After deployment, heroku gives you the URL to hit the web API.\n",
    "\n",
    "Once your application is deployed successfully, enter the command heroku logs --tail to see the logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14)\tWhat challenges you faced while deploying the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " some time command related issue\n",
    " \n",
    " some time memory computation issue\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)\tWhat is Cross validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross validation is resampling technique with a bsic idea of dividing the data set into two parts i.e train and test.on one part you try to train the model and on other i.e test which is unseen data for model.  makes prediction on it\n",
    "cheks well how well  our model perform.if the model works with good on both training and test  give good accuracy them its good model not overfitted or underfitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)\tWhy do we need to implement Cross validation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We tried to find the accuracy of the trained model using the same training data and found the accuracy to be 95% or maybe even 100%.\n",
    " our model has trained itself on the given data, i.e. it knows the data and it has generalized over it very well. But when you try and predict over a new set of data, it’s most likely to give you very bad accuracy, because it has never seen the data before and thus it fails to generalizes well over it. This is the problem of overfitting. To tackle such problem, Cross-validation is implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)\tWhat are different types of CV methods?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hold Out Method:\n",
    "It is the most basic of the CV techniques. It simply divides the dataset into two sets of training and test. The training dataset is used to train the model and then test data is fitted in the trained model to make predictions. We check the accuracy and assess our model on that basis. This method is used as it is computationally less costly. But the evaluation based on the Hold-out set can have a high variance because it depends heavily on which data points end up in the training set and which in test data. The evaluation will be different every time this division changes.\n",
    "\n",
    "\n",
    "k-fold Cross-Validation\n",
    "To tackle the high variance of Hold-out method, the k-fold method is used. The idea is simple, divide the whole dataset into ‘k’ sets preferably of equal sizes. Then the first set is selected as the test set and the rest ‘k-1’ sets are used to train the data. Error is calculated for this particular dataset. Then the steps are repeated, i.e. the second set is selected as the test data, and the remaining ‘k-1’ sets are used as the training data. Again, the error is calculated. Similarly, the process continues for ‘k’ times. In the end, the CV error is given as the mean of the total errors calculated individually, mathematically given as:  CV = 1/k submittion( MSE)\n",
    "\n",
    "The variance in error decreases with the increase in ‘k’. The disadvantage of k-fold cv is that it is computationally expensive as the algorithm runs from scratch for ‘k’ times.\n",
    "\n",
    "Leave One Out Cross Validation (LOOCV)\n",
    "OOCV is a special case of k-fold CV, where k becomes equal to n (number of observations). So instead of creating two subsets, it selects a single observation as a test data and rest of data as the training data. The error is calculated for this test observations. Now, the second observation is selected as test data, and the rest of the data is used as the training set. Again, the error is calculated for this particular test observation. This process continues ‘n’ times and in the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4)\tHow bias and variance varies for each CV method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while using the kfold cv we are facing high bais approach because \n",
    "performing k-fold CV for, say, k = 5 or k = 10 will lead to an intermediate level of bias since each training set contains (k − 1)n/k observations—fewer than in the LOOCV approach, but substantially more than in the validation set approach. Therefore, from the perspective of bias reduction, it is clear that LOOCV is to be preferred to k-fold CV.\n",
    "\n",
    "In the LOOCV we know that bias is not the only source for concern in an estimating procedure; we must also consider the procedure’s variance. It turns out that LOOCV has higher variance than does k-fold CV with k < n. Why is this the case? When we perform LOOCV, we are in effect averaging the outputs of n fitted models, each of which is trained on an almost identical set of observations; therefore, these outputs are highly (positively) correlated with each other. In contrast, when we perform k-fold CV with k < n, we are averaging the outputs of k fitted models that are somewhat less correlated with each other since the overlap between the training sets in each model is smaller. Since the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from k-fold CV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5)\tIs Train Test Split a kind of CV? True or False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "6)\tHow can we check over fitting using CV?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can check that our model is overfitting comapring the accuracy of trainig and testing.\n",
    " cross_val_score gives us the testing accuracy \n",
    "so we can compare the training score (return_train_score  libary command)  with \n",
    "cross_val_test score to show if our model is overfitted or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
