{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tExplain the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase in computation time: Majority of the machine learning algorithms they rely on the calculation of distance for model building and as the number of dimensions increases it becomes more and more computation-intensive to create a model out of it. For example, if we have to calculate the distance between two points in just one dimension, like two points on the number line, weâ€™ll just subtract the coordinate of one point from another and then take the magnitude:\n",
    "Distance=  ğ‘¥1âˆ’ğ‘¥2 \n",
    "What if we need to calculate the distance between two points in two dimensions?\n",
    "\n",
    "The same formula translates to: Distance=  (ğ‘¥1âˆ’ğ‘¥2)2+(ğ‘¦1âˆ’ğ‘¦2)2â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯âˆš \n",
    "What if we need to calculate the distance between two points in three dimensions?\n",
    "\n",
    "The same formula translates to: Distance=  (ğ‘¥1âˆ’ğ‘¥2)2+(ğ‘¦1âˆ’ğ‘¦2)2+(ğ‘§1âˆ’ğ‘§2)2â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯âˆš \n",
    "And for N-dimensions, the formula becomes: Distance= (ğ‘1âˆ’ğ‘2)2+(ğ‘1âˆ’ğ‘2)2+(ğ‘1âˆ’ğ‘2)2+â€¦+(ğ‘›1âˆ’ğ‘›2)2â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯âˆš \n",
    "This is the effort of calculating the distance between two points. Just imagine the number of calculations involved for all the data points involved.\n",
    "\n",
    "One more point to consider is that as the number of dimension increases, points are going far away from each other. This means that any new point that comes when we are testing the model is going to be farther away from our training points. This leads to a less reliable model, and it makes our model overfitted to the training data.\n",
    "\n",
    "\n",
    "Hard (or almost impossible) to visualise the relationship between features: As stated above, humans can not comprehend things beyond three dimensions. So, if we have an n-dimensional dataset, the only solution left to us is to create either a 2-D or 3-D graph out of it. Letâ€™s say for simplicity, we are creating 2-D graphs. Suppose we have 1000 features in the dataset. That results in a total (1000*999)/2= 499500 combinations possible for creating the 2-D graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tWhat is a dimensionality reduction technique?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction is a feature selection technique using which we reduce the number of features to be used for making a model without losing a significant amount of information compared to the original dataset. In other words, a dimensionality reduction technique projects a data of higher dimension to a lower-dimensional subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tExplain PCA. What are the principal components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principal component analysis is an unsupervised machine learning algorithm used for feature selection using dimensionality reduction techniques. As the name suggests, it finds out the principal components from the data. PCA transforms and fits the data from a higher-dimensional space to a new, lower-dimensional subspace This results into an entirely new coordinate system of the points where the first axis corresponds to the first principal component that explains the most variance in the data.\n",
    "\n",
    "Principal components are the derived features which explain the maximum variance in the data. The first principal component explains the most variance, the 2nd a bit less and so on. Each of the new dimensions found using PCA is a linear combination of the old features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tWhat is explained variance ratio?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the optimum number of Principal components needed are derived by  using the explained variance ratio. It represents the amount of variance each principal component is able to explain.\n",
    "\n",
    "or example, suppose if the square of distances of all the points from the origin that lie on PC1 is 50 and for the points on PC2 itâ€™s 5.\n",
    "\n",
    "EVR of PC1= ğ·ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’ğ‘œğ‘“ğ‘ƒğ¶1ğ‘ğ‘œğ‘–ğ‘›ğ‘¡ğ‘ /(ğ·ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’ğ‘œğ‘“ğ‘ƒğ¶1ğ‘ğ‘œğ‘–ğ‘›ğ‘¡ğ‘ +ğ·ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’ğ‘œğ‘“ğ‘ƒğ¶2ğ‘ğ‘œğ‘–ğ‘›ğ‘¡ğ‘ )=5055=0.91 \n",
    "EVR of PC2= ğ·ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’ğ‘œğ‘“ğ‘ƒğ¶2ğ‘ğ‘œğ‘–ğ‘›ğ‘¡ğ‘ /(ğ·ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’ğ‘œğ‘“ğ‘ƒğ¶1ğ‘ğ‘œğ‘–ğ‘›ğ‘¡ğ‘ +ğ·ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’ğ‘œğ‘“ğ‘ƒğ¶2ğ‘ğ‘œğ‘–ğ‘›ğ‘¡ğ‘ )=555=0.09 \n",
    "\n",
    "Thus PC1 explains 91% of the variance of data. Whereas, PC2 only explains 9% of the variance. Hence we can use only PC1 as the input for our model as it explains the majority of the variance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tWhat is a scree plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scree plots are the graphs that convey how much variance is explained by corresponding Principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tHow is the optimum number of principal components obtained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "principal component analysis does a SVD on a matrix and then generates an eigen value matrix. To select the principal components we have to take only the first few eigen values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tWhat is covariance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance is a measure for how two variables are related to each other, i.e., how two variables vary with each other.\n",
    "\n",
    "If the covariance is high, it means that the variables are highly correlated and change in one results in a change in the other one too. Generally, we avoid using highly correlated variables in building a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tWhat is the transpose of a matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transpose of a matrix is simply a flipped version of the original matrix. We can transpose a matrix by switching its rows with its columns. We denote the transpose of matrix A by AT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tWhat is an Eigen value and Eigen Vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvalues are a special set of scalars associated with a linear system of equations (i.e., a matrix equation) that are sometimes also known as characteristic roots, characteristic values (Hoffman and Kunze 1971), proper values, or latent roots\n",
    "\n",
    "If the new transformed vector is just a scaled form of the original vector then the original vector is known to be an eigenvector of the original matrix. Vectors that have this characteristic are special vectors and they are known as eigenvectors. Eigenvectors can be used to represent a large dimensional matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.\tWhy the Principal Components are orthogonal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principal components are the eigenvectors of a covariance matrix, and hence they are orthogonal.\n",
    "\n",
    "the dataset on which PCA technique is to be used must be scaled. The results are also sensitive to the relative scaling. As a layman, it is a method of summarizing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.\tExplain the Eigen Decomposition approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of Eigen-decomposition Approach\n",
    "\n",
    "Normalize columns of  ğ´  so that each feature has a mean of zero\n",
    "\n",
    "Compute sample covariance matrix  Î£=ğ´ğ‘‡ğ´/(ğ‘šâˆ’1) \n",
    "\n",
    "Perform eigen-decomposition of  Î£  using np.linalg.eig(Sigma)\n",
    "\n",
    "Compress by ordering  ğ‘˜  evectors according to largest e-values and compute  ğ´ğ‘‹ğ‘˜ \n",
    "\n",
    "Reconstruct from the compressed version by computing  ğ´ğ‘‹ğ‘˜ğ‘‹ğ‘‡ğ‘˜Summary of Eigen-decomposition Approach\n",
    "\n",
    "Normalize columns of  ğ´  so that each feature has a mean of zero\n",
    "\n",
    "Compute sample covariance matrix  Î£=ğ´ğ‘‡ğ´/(ğ‘šâˆ’1) \n",
    "\n",
    "Perform eigen-decomposition of  Î£  using np.linalg.eig(Sigma)\n",
    "\n",
    "Compress by ordering  ğ‘˜  evectors according to largest e-values and compute  ğ´ğ‘‹ğ‘˜ \n",
    "\n",
    "Reconstruct from the compressed version by computing  ğ´ğ‘‹ğ‘˜ğ‘‹ğ‘‡ğ‘˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.\tHow can PCA be used for data compression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA can be used for a sort of data compression as well. Using a smaller value of n_components allows you to represent a higher dimensional point as a sum of just a few principal component vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.\tDiscuss the pros and cons of PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros of PCA :-\n",
    "\n",
    "Correlated features are removed.\n",
    "\n",
    "Model training time is reduced.\n",
    "\n",
    "Overfitting is reduced.\n",
    "\n",
    "Helps in better visualizations\n",
    "\n",
    "Ability to handle noise\n",
    "\n",
    "Cons of PCA :-\n",
    "\n",
    "The resultant principal components are less interpretable than the original data\n",
    "\n",
    "Can lead to information loss if the explained variance threshold is not considered appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
