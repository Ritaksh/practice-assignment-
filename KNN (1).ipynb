{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)\tExplain the working of KNN algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "knn is a type of supervised learning algorithm which is used for both regression and classification.a dataset with different classes, knn tries to predict the correct class of the test data by calculating the distances between the test data to all the training points.it then select the  k points which are closest to the test data.once the points are selected the algorithm calculates the probability(in case of classification) of the test point belonging to the case of the k training points and the class with the highest probability is selected.in case of a regression problem the predicted value is the mean of the k selected training points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)\tHow KNN predicts the output for regression and classification problems?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a training dataset as given below. We have a new test data that we need to assign to one of the two classes.\n",
    "\n",
    "Now, the k-NN algorithm calculates the distance between the test data and the given training.\n",
    "\n",
    " After calculating the distance, it will select the k training points which are nearest to the test data. Let’s assume the value of k is 3 for our example.\n",
    " \n",
    "Now, 3 nearest neighbors are selected,Let’s see in which class our test data will be assigned :\n",
    "\n",
    "Number of Green class values = 2 Number of Red class values = 1 Probability(Green) = 2/3 Probability(Red) = 1/3\n",
    "\n",
    "Since the probability for Green class is higher than Red, the k-NN algorithm will assign the test data to the Green class.\n",
    "\n",
    "Similarly, if this were the case of a regression problem, the predicted value for the test data will simply be the mean of all the 3 nearest values.\n",
    "\n",
    "This is the basic working algorithm for k-NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)\tWhat are the different distances used in KNN? How are they calculated?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Euclidean Distance:\n",
    "It is the most commonly used method to calculate the distance between two points. The Euclidean distance between two points ‘p(p1,p2)’ and ‘q(q1,q2)’ is calculated as :\n",
    "\n",
    "d(p,q) = sqrt((q1-p1)^2 + (q2-p2)^2)\n",
    "\n",
    "\n",
    "Hamming distance\n",
    "A/c to Wikipedia, hamming distance is a distance metric that measures the number of mismatches between two vectors. It is mostly used in the case of categorical data.\n",
    "\n",
    "HamD(x,y) = sumititon(1)\n",
    "\n",
    "Generally, if we have features as categorical data then we consider the difference to be 0 if both the values are the same and the difference is 1 if both the values are different.\n",
    "\n",
    "Manhattan Distance\n",
    "\n",
    "This distance represents the sum of the absolute differences between the opposite values in vectors.\n",
    "\n",
    "MD(x,y) = sumition|x1-y1|\n",
    "\n",
    "Manhattan Distance is less influenced by outliers than the Euclidean distance. With very high dimensional data it is more preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4)\tWhat are Lazy Learners? Why KNN is called a lazy learner?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-NN algorithms are often termed as Lazy learners. Let’s understand why is that. Most of the algorithms like Bayesian classification, logistic regression, SVM etc., are called Eager learners. These algorithms generalize over the training set before receiving the test data i.e. they create a model based on the training data before receiving the test data and then do the prediction/classification on the test data. But this is not the case with the k-NN algorithm. It doesn’t create a generalized model for the training set but waits for the test data. Once test data is provided then only it starts generalizing the training data to classify the test data. So, a lazy learner just stores the training data and waits for the test set. Such algorithms work less while training and more while classifying a given test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)\tHow do we select the value of k? How bias and variance varies with k?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of k affects the k-NN classifier drastically. The flexibility of the model decreases with the increase of ‘k’. With lower value of ‘k’ variance is high and bias is low but as we increase the value of ‘k’ variance starts decreasing and bias starts increasing. With very low values of ‘k’ there is a chance of algorithm overfitting the data whereas with very high value of ‘k’ there is a chance of underfitting. Let’s visualize the trade-off between ‘1/k’, train error rate and test error rate\n",
    "\n",
    "We can see that the train error rate increases with the increase in the value of ‘k’ whereas test error rate decreases initially and then increases again. So, our goal should be to choose such value of ‘k’ for which we get a minimum of both the errors and avoid overfitting as well as underfitting. We use different ways to calculate the optimum value of ‘k’ such as cross validation, error versus k curve, checking accuracy for each value of ‘k’ etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6)\tWhat are advantages and disadvantages of KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "\n",
    "It can be used for both regression and classification problems.\n",
    "\n",
    "It is very simple and easy to implement.\n",
    "\n",
    "Mathematics behind the algorithm is easy to understand.\n",
    "\n",
    "There is no need to create model or do hyperparameter tuning.\n",
    "\n",
    "KNN doesn't make any assumption for the distribution of the given data.\n",
    "\n",
    "There is not much time cost in training phase.\n",
    "\n",
    "Cons:\n",
    "\n",
    "Finding the optimum value of ‘k’\n",
    "\n",
    "It takes a lot of time to compute the distance between each test sample and all training samples.\n",
    "\n",
    "Since the model is not saved beforehand in this algorithm (lazy learner), so every time one predicts a test value, it follows the same steps again and again.\n",
    "\n",
    "Since, we need to store the whole training set for every test set, it requires a lot of space.\n",
    "\n",
    "It is not suitable for high dimensional data.\n",
    "\n",
    "Expensive in testing phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7)\tDiscuss kDTree algorithm used for KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-d tree is a hierarchical binary tree. When this algorithm is used for k-NN classficaition, it rearranges the whole dataset in a binary tree structure, so that when test data is provided, it would give out the result by traversing through the tree, which takes less time than brute search.\n",
    "\n",
    "The dataset is divided like a tree as shown in the above figure. Say we have 3 dimensional data i.e. (x,y,z) then the tree is formed with root node being one of the dimensions, here we start with ‘x’. Then on the next level the split is done on basis of the second dimension, ‘y’ in our case. Similarly, third level with 3rd dimension and so on. And in case of ‘k’ dimensions, each split is made on basis of ‘k’ dimensions.\n",
    "\n",
    "Once the tree is formed , it is easy for algorithm to search for the probable nearest neighbor just by traversing the tree. The main problem k-d trees is that it gives probable nearest neighbors but can miss out actual nearest neighbors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "8)\tDiscuss Ball Tree algorithm used for KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to k-d trees, Ball trees are also hierarchical data structure. These are very efficient specially in case of higher dimensions.\n",
    "\n",
    "These are formed by following steps:\n",
    "\n",
    "Two clusters are created initially\n",
    "All the data points must belong to atleast one of the clusters.\n",
    "3) One point cannot be in both clusters.\n",
    "Distance of the point is calculated from the centroid of the each cluster. The point closer to the centroid goes into that particular cluster.\n",
    "Each cluster is then divided into sub clusters again, and then the points are classified into each cluster on the basis of distance from centroid.\n",
    "This is how the clusters are kept to be divided till a certain depth\n",
    "\n",
    "Ball tree formation initially takes a lot of time but once the nested clusters are created, finding nearest neighbors is easier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
