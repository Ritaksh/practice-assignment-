{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWhat is Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "boosting is an ensemble approach thats starts from a weaker decision and keeps on building \n",
    "the models on such that the final prediction is the weighted sum of all the weaker \n",
    "decision -makers. the weights are assigned based on the performance of an individual tree\n",
    "\n",
    "ensemble parameters are calculated in stagewise way which means that the while calculating \n",
    "the subsequent weights, the learning from previous tree is considered as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tHow do boosting and bagging differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bagging is a way to decrease the variance in the prediction by generating additional data for training\n",
    "from dataset using combinations with repetitions to produce  multi sets of the original data.\n",
    "boosting is an iterative technique which adjusts the weight of an observation \n",
    "based on the last classification.\n",
    "if an observation was classified incorrectly it tries to increase the weight of the observation\n",
    "boosting in genreal builds strong predictive models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tWhat are week and strong classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weak classifier  are classifier which perform only slightly better than a random\n",
    "classifier.these have the some clue on which how to predict   the right labels,\n",
    "but much strong clasifier.\n",
    "\n",
    "strong classifer which have the right informtion to predict the right labels of outcome\n",
    "or strong classifier are combination of week classifier which drive the right outcomme labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tWhy are trees deemed fit for boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any algorithm could have been used as a base for the boosting technique, but the reason for choosing trees are:\n",
    " \n",
    " computationaly  scalability\n",
    " \n",
    " handles missing values\n",
    " \n",
    " robust to outliers\n",
    " \n",
    " doen not require feature scaling\n",
    " \n",
    " can deal with the irrelevant inputs \n",
    " \n",
    " interpretable if small\n",
    " \n",
    " handles mixed predictors as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tExplain the step by step implementation of ADA Boost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that the number of training samples is denoted by  𝑁 , and the number of iterations (created trees) is  𝑀 . Notice that possible class outputs are  𝑌={−1,1} \n",
    "\n",
    "Initialize the observation weights  𝑤𝑖=1/𝑁    where  𝑖=1,2,…,𝑁  for all the samples.\n",
    "\n",
    "For  𝑚=1  to  𝑀 :\n",
    "\n",
    "fit a classifier  𝐺𝑚(𝑥)  to the training data using weights  𝑤𝑖 ,\n",
    "\n",
    "compute  𝑒𝑟𝑟𝑚=∑𝑁𝑖=1𝑤𝑖𝐼(𝑦𝑖≠𝐺𝑚(𝑥))/∑𝑁𝑖=1𝑤𝑖 ,\n",
    "\n",
    "compute  𝛼𝑚=12log((1−𝑒𝑟𝑟𝑚)/𝑒𝑟𝑟𝑚) . This is the contribution of that tree to the final result.\n",
    "\n",
    "calculate the new weights using the formula:\n",
    "\n",
    "𝑤𝑖←𝑤𝑖⋅exp[𝛼𝑚⋅𝐼(𝑦𝑖≠𝐺𝑚(𝑥)] , where  𝑖=1,2,…,𝑁 \n",
    "\n",
    "Normalize the new sample weights so that their sum is 1.\n",
    "\n",
    "Construct the next tree using the new weights\n",
    "\n",
    "At the end, compare the summation of results from all the trees and the final result is either the one with the highest sum(for regression) or it is the class which has the most weighted voted average(for classification).\n",
    "\n",
    "Output  𝐺𝑚(𝑥)=𝑎𝑟𝑔𝑚𝑎𝑥[∑𝑀𝑚=1𝛼𝑚𝐺𝑚(𝑥)]  (Regression)\n",
    "\n",
    "Output  𝐺𝑚(𝑥)=𝑠𝑖𝑔𝑚[∑𝑀𝑚=1𝛼𝑚𝐺𝑚(𝑥)]  (Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tWhat are pseudo residuals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pseduo residuals are  difference between the actual labels and the average  of the firt prediction (the predicted result)\n",
    "\n",
    "derivative of the pseduo residual are = (𝛿𝐿(𝑦𝑖,𝑓(𝑥𝑖))/𝛿(𝑓(𝑥𝑖)))\n",
    "\n",
    "where, L is the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tExplain the step by step implementation of Gradient boosted trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the average of the label column as initially this average shall minimise the total error.\n",
    "\n",
    "Calculate the pseudo residuals.\n",
    "\n",
    " Pseudo residual= actual label- the predicted result (which is average in the first iteration)\n",
    " \n",
    " derivative of the pseudo residual= (𝛿𝐿(𝑦𝑖,𝑓(𝑥𝑖))𝛿(𝑓(𝑥𝑖))) \n",
    "\n",
    "where, L is the loss function.\n",
    "\n",
    " Here, the gradient of the error term is getting calculated as the goal is to \n",
    " minimize the error. Hence the name gradient boosted trees\n",
    "\n",
    "create a tree to predict the pseudo residuals instead of a tree to predict for \n",
    "the actual column values.\n",
    "\n",
    "new result= previous result+learning rate* residual\n",
    "\n",
    "Mathematically,  𝐹1(𝑥)=𝐹0(𝑥)+𝜈∑𝛾 \n",
    "\n",
    "where  𝜈  is the learning rate and  𝛾  is the residual\n",
    "\n",
    "Repeat these steps until the residual stops decreasing\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tExplain the step by step implementation of XGBoost Algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the other tree-building algorithms, XGBoost doesn’t use entropy or Gini indices. Instead, it utilises gradient (the error term) and hessian for creating the trees. Hessian for a Regression problem is the number of residuals and for a classification problem. Mathematically, Hessian is a second order derivative of the loss at the current estimate given as:\n",
    "\n",
    "\n",
    "where L is the loss function.\n",
    "\n",
    "Initialise the tree with only one leaf.\n",
    "\n",
    "compute the similarity using the formula\n",
    "\n",
    "𝑆𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦=𝐺𝑟𝑎𝑑𝑖𝑒𝑛𝑡2/ℎ𝑒𝑠𝑠𝑖𝑎𝑛+𝜆\n",
    " \n",
    "Where  𝜆  is the regularisation term.\n",
    "\n",
    "Now for splitting data into a tree form, calculate\n",
    "\n",
    "𝐺𝑎𝑖𝑛=𝑙𝑒𝑓𝑡𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦+𝑟𝑖𝑔ℎ𝑡𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦−𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦𝑓𝑜𝑟𝑟𝑜𝑜𝑡\n",
    " \n",
    "For tree pruning, the parameter  𝛾  is used. The algorithm starts from the lowest level of the tree and then starts pruning based on the value of  𝛾 .\n",
    "\n",
    "If  𝐺𝑎𝑖𝑛−𝛾<0 , remove that branch. Else, keep the branch\n",
    "\n",
    "Learning is done using the equation\n",
    "\n",
    "𝑁𝑒𝑤𝑉𝑎𝑙𝑢𝑒=𝑜𝑙𝑑𝑉𝑎𝑙𝑢𝑒+𝜂∗𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛\n",
    " \n",
    "where  𝜂  is the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tWhat are the advantages of XGBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "good to control bais-variance trade off\n",
    "\n",
    "computation speed is high as it utilize parallel computing and cache optimization\n",
    "\n",
    "uses hardware otpimization\n",
    "\n",
    "works well even if the feature are correlated\n",
    "\n",
    "robust eeven if there is noise in the classification problem\n",
    "\n",
    "the facility of early stoping\n",
    "\n",
    "the package is evolving i.e new features are being added"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
